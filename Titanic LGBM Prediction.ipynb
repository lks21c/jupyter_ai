{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3aaaa23",
   "metadata": {},
   "source": [
    "# Titanic LGBM Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde0c4c",
   "metadata": {},
   "source": [
    "## 탐색적 데이터 분석 (EDA) 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b78c4e-a471-4ec7-b2ab-dca1c21815ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (2529827067.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('a\")\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "print('a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbeb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NOTE: 향후 분석 진행 시, 실제로 사용되지 않는 라이브러리는 제거하여 메모리 사용량을 미세하게 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global plot style for better aesthetics and readability\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10 # Base font size\n",
    "plt.rcParams['axes.titlesize'] = 14 # Title font size\n",
    "plt.rcParams['axes.labelsize'] = 12 # Label font size\n",
    "plt.rcParams['xtick.labelsize'] = 10 # X-tick label size\n",
    "plt.rcParams['ytick.labelsize'] = 10 # Y-tick label size\n",
    "plt.rcParams['legend.fontsize'] = 10 # Legend font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d501ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Assuming 'train.csv' is in the same directory or specified path\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fef825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Exploratory Data Analysis (EDA) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_basic_info(df):\n",
    "    \"\"\"\n",
    "    Prints basic information about the DataFrame, including its structure,\n",
    "    first few rows, and statistical summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 1. 데이터 기본 정보 확인 ---\")\n",
    "    print(\"\\n데이터셋 정보:\")\n",
    "    df.info()\n",
    "    print(\"\\n데이터셋의 처음 5행:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n데이터셋의 통계적 요약:\")\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74467c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identifies and visualizes missing values across DataFrame columns.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. 결측치 확인 및 시각화 ---\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percentage = (missing_data / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing Percentage (%)': missing_percentage\n",
    "    })\n",
    "    missing_info = missing_info[missing_info['Missing Count'] > 0].sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "    if not missing_info.empty:\n",
    "        print(\"\\n결측치 정보:\")\n",
    "        print(missing_info)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=missing_info.index, y='Missing Percentage (%)', data=missing_info, palette='viridis')\n",
    "        plt.title('각 컬럼별 결측치 비율')\n",
    "        plt.ylabel('결측치 비율 (%)')\n",
    "        plt.xlabel('컬럼명')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"결측치가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06002983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_variable(df, target_col):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes the distribution of the target variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 3. 타겟 변수 ({target_col}) 분포 확인 ---\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x=target_col, data=df, palette='pastel')\n",
    "    plt.title(f'{target_col} 분포')\n",
    "    plt.xlabel('생존 여부 (0: 사망, 1: 생존)')\n",
    "    plt.ylabel('승객 수')\n",
    "    plt.xticks(ticks=[0, 1], labels=['사망', '생존'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    survival_rate = df[target_col].value_counts(normalize=True) * 100\n",
    "    if 0 in survival_rate.index and 1 in survival_rate.index:\n",
    "        print(f\"사망자 비율: {survival_rate[0]:.2f}%\")\n",
    "        print(f\"생존자 비율: {survival_rate[1]:.2f}%\")\n",
    "    elif 0 in survival_rate.index:\n",
    "        print(f\"사망자 비율: {survival_rate[0]:.2f}%\")\n",
    "        print(\"생존자 없음.\")\n",
    "    elif 1 in survival_rate.index:\n",
    "        print(\"사망자 없음.\")\n",
    "        print(f\"생존자 비율: {survival_rate[1]:.2f}%\")\n",
    "    else:\n",
    "        print(\"타겟 변수에 0 또는 1 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2366af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_analysis(df, categorical_features, numerical_features):\n",
    "    \"\"\"\n",
    "    Performs univariate analysis for categorical (count plots) and\n",
    "    numerical (histograms and box plots) features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 4. 각 특징들의 분포 시각화 (유니바리에이트 분석) ---\")\n",
    "\n",
    "    if categorical_features:\n",
    "        print(\"\\n4.1. 범주형 특징 분포:\")\n",
    "        n_cat = len(categorical_features)\n",
    "        n_cols_cat = 3\n",
    "        n_rows_cat = (n_cat + n_cols_cat - 1) // n_cols_cat # Ceiling division\n",
    "        fig_cat, axes_cat = plt.subplots(n_rows_cat, n_cols_cat, figsize=(n_cols_cat * 6, n_rows_cat * 5))\n",
    "        axes_cat = axes_cat.flatten() # Ensure axes is a 1D array even for single row/column\n",
    "\n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            if i < len(axes_cat):\n",
    "                sns.countplot(x=feature, data=df, ax=axes_cat[i], palette='coolwarm')\n",
    "                axes_cat[i].set_title(f'{feature} 분포')\n",
    "                axes_cat[i].set_ylabel('승객 수')\n",
    "                axes_cat[i].set_xlabel(feature)\n",
    "\n",
    "        # Remove unused subplots if any\n",
    "        for j in range(n_cat, len(axes_cat)):\n",
    "            fig_cat.delaxes(axes_cat[j])\n",
    "\n",
    "        plt.suptitle('범주형 특징 분포', y=1.02, fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust rect to prevent title overlap\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n4.1. 분석할 범주형 특징이 없습니다.\")\n",
    "\n",
    "    if numerical_features:\n",
    "        print(\"\\n4.2. 수치형 특징 분포 (히스토그램 및 박스 플롯):\")\n",
    "        n_num = len(numerical_features)\n",
    "        fig_num, axes_num = plt.subplots(n_num, 2, figsize=(15, 5 * n_num))\n",
    "        axes_num = axes_num.flatten()\n",
    "\n",
    "        for i, feature in enumerate(numerical_features):\n",
    "            # Histplot\n",
    "            sns.histplot(df[feature].dropna(), kde=True, ax=axes_num[2*i], bins=30, color='skyblue')\n",
    "            axes_num[2*i].set_title(f'{feature} 분포 (히스토그램)')\n",
    "            axes_num[2*i].set_xlabel(feature)\n",
    "            axes_num[2*i].set_ylabel('밀도 / 승객 수')\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(y=df[feature].dropna(), ax=axes_num[2*i+1], color='lightcoral')\n",
    "            axes_num[2*i+1].set_title(f'{feature} 분포 (박스 플롯)')\n",
    "            axes_num[2*i+1].set_ylabel(feature)\n",
    "\n",
    "        plt.suptitle('수치형 특징 분포', y=1.02, fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n4.2. 분석할 수치형 특징이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca95b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_analysis(df, target_col, categorical_features, numerical_features):\n",
    "    \"\"\"\n",
    "    Analyzes relationships between features and the target variable ('Survived').\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 5. {target_col}와 다른 특징들 간의 관계 분석 (바이바리에이트 분석) ---\")\n",
    "\n",
    "    if categorical_features:\n",
    "        print(f\"\\n5.1. 범주형 특징 vs. {target_col}:\")\n",
    "        n_cat = len(categorical_features)\n",
    "        n_cols_cat = 3\n",
    "        n_rows_cat = (n_cat + n_cols_cat - 1) // n_cols_cat\n",
    "        fig_cat, axes_cat = plt.subplots(n_rows_cat, n_cols_cat, figsize=(n_cols_cat * 6, n_rows_cat * 5))\n",
    "        axes_cat = axes_cat.flatten()\n",
    "\n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            if i < len(axes_cat):\n",
    "                sns.barplot(x=feature, y=target_col, data=df, ax=axes_cat[i], palette='viridis', errorbar=None)\n",
    "                axes_cat[i].set_title(f'{feature}별 {target_col}율')\n",
    "                axes_cat[i].set_ylabel(f'{target_col}율')\n",
    "                axes_cat[i].set_xlabel(feature)\n",
    "\n",
    "        for j in range(n_cat, len(axes_cat)):\n",
    "            fig_cat.delaxes(axes_cat[j])\n",
    "\n",
    "        plt.suptitle(f'범주형 특징별 {target_col}율', y=1.02, fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\n5.1. 분석할 범주형 특징이 없습니다.\")\n",
    "\n",
    "    if numerical_features:\n",
    "        print(f\"\\n5.2. 수치형 특징과 {target_col} 관계:\")\n",
    "        n_num = len(numerical_features)\n",
    "        fig_num, axes_num = plt.subplots(n_num, 2, figsize=(15, 6 * n_num))\n",
    "        axes_num = axes_num.flatten()\n",
    "\n",
    "        for i, feature in enumerate(numerical_features):\n",
    "            # KDE plot\n",
    "            sns.kdeplot(x=feature, data=df, hue=target_col, fill=True, common_norm=False, ax=axes_num[2*i], palette='mako')\n",
    "            axes_num[2*i].set_title(f'{feature} 분포 vs. {target_col}')\n",
    "            axes_num[2*i].set_xlabel(feature)\n",
    "            axes_num[2*i].set_ylabel('밀도')\n",
    "            # Custom legend labels for Survived (0: 사망, 1: 생존)\n",
    "            handles, labels = axes_num[2*i].get_legend_handles_labels()\n",
    "            if '0' in labels and '1' in labels: # Check if default labels exist before modifying\n",
    "                labels_map = {'0': '사망', '1': '생존'}\n",
    "                new_labels = [labels_map.get(l, l) for l in labels]\n",
    "                axes_num[2*i].legend(handles=handles, labels=new_labels, title=target_col)\n",
    "            else:\n",
    "                axes_num[2*i].legend(title=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e130a",
   "metadata": {},
   "source": [
    "\n",
    "            # Box plot\n",
    "            sns.boxplot(x=target_col, y=feature, data=df, ax=axes_num[2*i+1], palette='rocket')\n",
    "            axes_num[2*i+1].set_title(f'{target_col}별 {feature} 분포')\n",
    "            axes_num[2*i+1].set_xlabel('생존 여부 (0: 사망, 1: 생존)')\n",
    "            axes_num[2*i+1].set_ylabel(feature)\n",
    "            axes_num[2*i+1].set_xticks(ticks=[0, 1], labels=['사망', '생존'])\n",
    "\n",
    "        plt.suptitle(f'수치형 특징별 {target_col} 관계', y=1.02, fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\n5.2. 분석할 수치형 특징이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a81261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df, target_col=None):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the correlation matrix for numerical features.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 6. 수치형 특징 간의 상관 관계 분석 ---\")\n",
    "\n",
    "    numeric_cols_for_corr = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'PassengerId' in numeric_cols_for_corr:\n",
    "        numeric_cols_for_corr.remove('PassengerId')\n",
    "\n",
    "    if not numeric_cols_for_corr:\n",
    "        print(\"상관 관계를 분석할 수치형 특징이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    correlation_matrix = df[numeric_cols_for_corr].corr()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar_kws={'label': '상관 계수'})\n",
    "    plt.title('수치형 특징 간의 상관 관계 매트릭스')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Main EDA Execution Flow ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- 탐색적 데이터 분석 (EDA) 시작 ---\")\n",
    "\n",
    "    # Define the target column and features to be excluded from general analysis\n",
    "    TARGET_COLUMN = 'Survived'\n",
    "    EXCLUDE_FEATURES = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "\n",
    "    # Explicitly define categorical and numerical features for this dataset\n",
    "    # These lists can be made dynamic based on data types and unique value counts if needed\n",
    "    categorical_features = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\n",
    "    numerical_features = ['Age', 'Fare']\n",
    "\n",
    "    # Filter features to ensure they exist in the DataFrame and are not target/excluded\n",
    "    available_categorical_features = [f for f in categorical_features if f in train_df.columns and f not in EXCLUDE_FEATURES and f != TARGET_COLUMN]\n",
    "    available_numerical_features = [f for f in numerical_features if f in train_df.columns and f not in EXCLUDE_FEATURES and f != TARGET_COLUMN]\n",
    "\n",
    "    # Check if the target column exists before proceeding with EDA\n",
    "    if TARGET_COLUMN not in train_df.columns:\n",
    "        print(f\"오류: 타겟 컬럼 '{TARGET_COLUMN}'이 데이터프레임에 존재하지 않습니다. EDA를 수행할 수 없습니다.\")\n",
    "    else:\n",
    "        display_basic_info(train_df)\n",
    "        analyze_missing_values(train_df)\n",
    "        analyze_target_variable(train_df, TARGET_COLUMN)\n",
    "        univariate_analysis(train_df, available_categorical_features, available_numerical_features)\n",
    "        bivariate_analysis(train_df, TARGET_COLUMN, available_categorical_features, available_numerical_features)\n",
    "        analyze_correlations(train_df) # analyze_correlations handles numeric column selection internally\n",
    "\n",
    "    print(\"\\n--- 탐색적 데이터 분석 (EDA) 완료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce19b0",
   "metadata": {},
   "source": [
    "## 특징 공학 (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41739f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(train_df_raw: pd.DataFrame, test_df_raw: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs feature engineering on Titanic train and test dataframes.\n",
    "\n",
    "    Args:\n",
    "        train_df_raw (pd.DataFrame): The raw training DataFrame, expected to contain\n",
    "                                     'PassengerId', 'Survived', 'Name', 'SibSp', 'Parch',\n",
    "                                     'Fare', 'Ticket', 'Cabin', 'Age', 'Embarked', 'Pclass', 'Sex'.\n",
    "                                     Basic missing value imputation (e.g., Age, Embarked)\n",
    "                                     is assumed to have been completed.\n",
    "        test_df_raw (pd.DataFrame): The raw test DataFrame, expected to contain\n",
    "                                    'PassengerId', 'Name', 'SibSp', 'Parch', 'Fare',\n",
    "                                    'Ticket', 'Cabin', 'Age', 'Embarked', 'Pclass', 'Sex'.\n",
    "                                    Basic missing value imputation (e.g., Age)\n",
    "                                    is assumed to have been completed.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:\n",
    "                                           - The engineered training DataFrame.\n",
    "                                           - The engineered test DataFrame.\n",
    "    \"\"\"\n",
    "    # Create copies to avoid modifying the original dataframes passed to the function\n",
    "    train_df = train_df_raw.copy()\n",
    "    test_df = test_df_raw.copy()\n",
    "\n",
    "    # Store 'PassengerId' and 'Survived' target as they are not features for engineering\n",
    "    train_passenger_ids = train_df['PassengerId']\n",
    "    test_passenger_ids = test_df['PassengerId']\n",
    "    train_target = train_df['Survived']\n",
    "\n",
    "    # Combine dataframes for consistent feature engineering\n",
    "    # Drop 'Survived' (only in train) and 'PassengerId' from both before combining\n",
    "    combined_df = pd.concat([train_df.drop(columns=['Survived', 'PassengerId']),\n",
    "                             test_df.drop(columns=['PassengerId'])],\n",
    "                            axis=0, ignore_index=True)\n",
    "\n",
    "    # Store the original length of the training data for later splitting\n",
    "    train_data_len = len(train_df)\n",
    "\n",
    "    # --- Feature Engineering Steps ---\n",
    "\n",
    "    # 1. Extract 'Title' from 'Name'\n",
    "    # Use str.extract with regex to get the title (e.g., Mr, Mrs, Miss)\n",
    "    combined_df['Title'] = combined_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Fill any NaNs (if a name doesn't match the regex pattern) with 'Unknown'\n",
    "    combined_df['Title'] = combined_df['Title'].fillna('Unknown')\n",
    "\n",
    "    # Standardize and group less common titles\n",
    "    # Use .loc for explicit assignment to avoid potential SettingWithCopyWarning\n",
    "    combined_df.loc[combined_df['Title'].isin(['Mlle', 'Ms']), 'Title'] = 'Miss'\n",
    "    combined_df.loc[combined_df['Title'] == 'Mme', 'Title'] = 'Mrs'\n",
    "    \n",
    "    # Define rare titles including the 'Unknown' placeholder and group them\n",
    "    rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev',\n",
    "                   'Sir', 'Jonkheer', 'Dona', 'Unknown']\n",
    "    combined_df.loc[combined_df['Title'].isin(rare_titles), 'Title'] = 'Rare'\n",
    "\n",
    "    # 2. Create 'FamilySize' from 'SibSp' and 'Parch'\n",
    "    # Family size includes the person themselves, so add 1\n",
    "    combined_df['FamilySize'] = combined_df['SibSp'] + combined_df['Parch'] + 1\n",
    "\n",
    "    # 3. Create 'IsAlone' feature\n",
    "    # 'IsAlone' is 1 if FamilySize is 1, else 0\n",
    "    combined_df['IsAlone'] = (combined_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # 4. Calculate 'FarePerPerson'\n",
    "    # Fill any missing 'Fare' values with the median of the combined dataset\n",
    "    combined_df['Fare'].fillna(combined_df['Fare'].median(), inplace=True)\n",
    "    # Calculate fare per person, FamilySize is guaranteed to be >= 1\n",
    "    combined_df['FarePerPerson'] = combined_df['Fare'] / combined_df['FamilySize']\n",
    "\n",
    "    # --- Drop original columns that are no longer needed or replaced ---\n",
    "    # 'Name' is replaced by 'Title'\n",
    "    # 'SibSp', 'Parch' are replaced by 'FamilySize' and 'IsAlone'\n",
    "    # 'Ticket', 'Cabin' are dropped for simplicity as per original code's intent\n",
    "    columns_to_drop = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']\n",
    "    combined_df = combined_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # --- Split the combined dataframe back into engineered train and test sets ---\n",
    "    train_df_fe = combined_df.iloc[:train_data_len].copy()\n",
    "    test_df_fe = combined_df.iloc[train_data_len:].copy()\n",
    "\n",
    "    # Re-add 'Survived' target and 'PassengerId' to the respective dataframes\n",
    "    train_df_fe['Survived'] = train_target\n",
    "    train_df_fe['PassengerId'] = train_passenger_ids\n",
    "    test_df_fe['PassengerId'] = test_passenger_ids\n",
    "\n",
    "    return train_df_fe, test_df_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf4fb",
   "metadata": {},
   "source": [
    "## 결측치 처리 (Missing Value Imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82470b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 확인 (재확인)\n",
    "print(\"--- 결측치 처리 전 ---\")\n",
    "print(\"Train 데이터셋 결측치:\\n\", train_df.isnull().sum())\n",
    "print(\"\\nTest 데이터셋 결측치:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e380d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리 값을 train 데이터셋에서 계산하여 데이터 누수 방지\n",
    "# 1. 'Age' 결측치 처리: 연속형 변수이므로 중앙값(median)으로 대체\n",
    "median_age = train_df['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd17fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 'Fare' 결측치 처리: 연속형 변수이므로 중앙값(median)으로 대체\n",
    "median_fare = train_df['Fare'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 'Embarked' 결측치 처리: 범주형 변수이므로 최빈값(mode)으로 대체\n",
    "most_frequent_embarked = train_df['Embarked'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a576579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 컬럼별 결측치 대체 값 딕셔너리 정의\n",
    "imputation_values = {\n",
    "    'Age': median_age,\n",
    "    'Fare': median_fare,\n",
    "    'Embarked': most_frequent_embarked\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae758052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df와 test_df에 일괄적으로 결측치 처리 적용\n",
    "for col, value in imputation_values.items():\n",
    "    train_df[col].fillna(value, inplace=True)\n",
    "    test_df[col].fillna(value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리 후 확인\n",
    "print(\"\\n--- 결측치 처리 후 ---\")\n",
    "print(\"Train 데이터셋 결측치:\\n\", train_df.isnull().sum())\n",
    "print(\"\\nTest 데이터셋 결측치:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b26e7",
   "metadata": {},
   "source": [
    "## 범주형 특징 인코딩 (Categorical Feature Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26548c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of categorical columns to be encoded.\n",
    "# 'Pclass' is treated as categorical.\n",
    "categorical_features = ['Sex', 'Embarked', 'Title', 'Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store PassengerId for the test set; it's needed for submission but not for training.\n",
    "test_passenger_ids = test_df['PassengerId'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable ('Survived') from the training features.\n",
    "y_train = train_df['Survived'].copy()\n",
    "X_train = train_df.drop('Survived', axis=1).copy()\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'PassengerId' from feature sets as it is an identifier, not a predictive feature.\n",
    "X_train = X_train.drop('PassengerId', axis=1)\n",
    "X_test = X_test.drop('PassengerId', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test to apply One-Hot Encoding uniformly.\n",
    "# This ensures consistent columns across both datasets after encoding.\n",
    "all_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65530cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to the specified categorical features.\n",
    "# `drop_first=False` is generally preferred for tree-based models like LightGBM.\n",
    "all_data_encoded = pd.get_dummies(all_data, columns=categorical_features, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined dataframe back into processed training and testing sets.\n",
    "X_train_processed = all_data_encoded.iloc[:len(X_train)].copy()\n",
    "X_test_processed = all_data_encoded.iloc[len(X_train):].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the global `train_df` and `test_df` variables with the encoded data.\n",
    "train_df = X_train_processed\n",
    "test_df = X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e35f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification and Display ---\n",
    "print(\"--- After Categorical Feature Encoding ---\")\n",
    "print(\"\\nTransformed train_df head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTransformed test_df head:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nShape of train_df after encoding: {train_df.shape}\")\n",
    "print(f\"Shape of test_df after encoding: {test_df.shape}\")\n",
    "print(f\"Number of columns in train_df: {len(train_df.columns)}\")\n",
    "print(f\"Number of columns in test_df: {len(test_df.columns)}\")\n",
    "print(f\"Number of target values (y_train): {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235317ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify column consistency between training and testing sets.\n",
    "if set(train_df.columns) == set(test_df.columns):\n",
    "    print(\"\\nSUCCESS: Columns of train_df and test_df are identical, ensuring consistent feature sets.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Columns of train_df and test_df are NOT identical. Please check the encoding process.\")\n",
    "    print(\"Train columns not in Test:\", set(train_df.columns) - set(test_df.columns))\n",
    "    print(\"Test columns not in Train:\", set(test_df.columns) - set(train_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2868e5",
   "metadata": {},
   "source": [
    "## 데이터 분할 및 스케일링 (Data Splitting & Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ac141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(train_df_raw, test_df_raw, \n",
    "                              target_column='Survived', \n",
    "                              columns_to_exclude_from_scaling=None,\n",
    "                              test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs feature/target separation, numerical feature scaling (excluding specified columns),\n",
    "    and data splitting.\n",
    "\n",
    "    Args:\n",
    "        train_df_raw (pd.DataFrame): Raw training data. Assumed to be already preprocessed\n",
    "                                     for categorical features (e.g., one-hot encoded).\n",
    "        test_df_raw (pd.DataFrame): Raw test data (without target). Assumed to be already\n",
    "                                    preprocessed for categorical features.\n",
    "        target_column (str): The name of the target variable column. Defaults to 'Survived'.\n",
    "        columns_to_exclude_from_scaling (list, optional): A list of column names that are\n",
    "                                                          numerical but should NOT be scaled.\n",
    "                                                          Defaults to ['PassengerId'] if None.\n",
    "        test_size (float): The proportion of the training data to include in the validation split.\n",
    "                           Defaults to 0.2.\n",
    "        random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "                            Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_val, y_train, y_val, X_test)\n",
    "               - X_train (pd.DataFrame): Features for training (scaled).\n",
    "               - X_val (pd.DataFrame): Features for validation (scaled).\n",
    "               - y_train (pd.Series): Target for training.\n",
    "               - y_val (pd.Series): Target for validation.\n",
    "               - X_test (pd.DataFrame): Processed features for the test set (scaled).\n",
    "    \"\"\"\n",
    "    if columns_to_exclude_from_scaling is None:\n",
    "        columns_to_exclude_from_scaling = ['PassengerId']\n",
    "\n",
    "    # Create copies to avoid modifying original dataframes and potential SettingWithCopyWarning\n",
    "    train_df = train_df_raw.copy()\n",
    "    X_test = test_df_raw.copy() # X_test will be processed from this copy\n",
    "\n",
    "    # --- 1. 특징(X)과 타겟(y) 분리 ---\n",
    "    # 'train_df'는 이전 단계에서 전처리된 훈련 데이터라고 가정합니다.\n",
    "    # 'TARGET'은 타겟 변수의 컬럼 이름이라고 가정합니다.\n",
    "    if target_column not in train_df.columns:\n",
    "        raise KeyError(f\"Target column '{target_column}' not found in train_df.\")\n",
    "\n",
    "    # 훈련 데이터에서 타겟 변수를 분리하여 X와 y를 생성합니다.\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "\n",
    "    # 'test_df'는 이전 단계에서 전처리된 테스트 데이터라고 가정합니다.\n",
    "    # 테스트 데이터는 타겟 변수가 없으므로 X_test로 바로 사용합니다.\n",
    "    # X와 X_test의 컬럼 일치를 보장하기 위해 reindex를 사용합니다.\n",
    "    # 훈련 세트에만 존재하는 범주형 컬럼이 테스트 세트에 없을 경우 0으로 채워 일치시킵니다.\n",
    "    # 테스트 세트에만 존재하는 컬럼은 제거합니다.\n",
    "    \n",
    "    missing_in_test = set(X.columns) - set(X_test.columns)\n",
    "    extra_in_test = set(X_test.columns) - set(X.columns)\n",
    "\n",
    "    if missing_in_test or extra_in_test:\n",
    "        print(\"Warning: Features in X and X_test do not perfectly match. Aligning X_test to X's columns.\")\n",
    "        if missing_in_test:\n",
    "            print(f\"Columns missing in X_test: {missing_in_test}. Adding with fill_value=0.\")\n",
    "        if extra_in_test:\n",
    "            print(f\"Columns extra in X_test: {extra_in_test}. Dropping these from X_test.\")\n",
    "        X_test = X_test.reindex(columns=X.columns, fill_value=0) # 0 for one-hot encoded features\n",
    "\n",
    "    print(f\"훈련 데이터 X shape: {X.shape}\")\n",
    "    print(f\"훈련 데이터 y shape: {y.shape}\")\n",
    "    print(f\"테스트 데이터 X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c6e35",
   "metadata": {},
   "source": [
    "\n",
    "    # --- 2. 숫자형 특징 스케일링 ---\n",
    "    # LightGBM은 트리 기반 모델이므로 스케일링이 필수적이지는 않지만,\n",
    "    # 다른 모델과의 비교나 특정 상황에서 성능 향상을 가져올 수 있습니다.\n",
    "    # 여기서는 일반적인 관행에 따라 숫자형 특징에 StandardScaler를 적용합니다.\n",
    "\n",
    "    # 숫자형 특징 컬럼을 식별합니다. (여기서는 'PassengerId'와 같은 ID 컬럼은 제외)\n",
    "    # 'Sex', 'Embarked' 등 범주형 컬럼은 이미 원-핫 인코딩이나 레이블 인코딩되었다고 가정합니다.\n",
    "    # 'Pclass'는 범주형으로 처리되었을 수 있지만, 여기서는 일단 숫자형으로 간주합니다.\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # 모델에 직접 사용되지 않는 ID 컬럼 등은 스케일링에서 제외할 수 있습니다.\n",
    "    # 예를 들어, 'PassengerId'가 X에 남아있다면 스케일링 대상에서 제외합니다.\n",
    "    scaled_numerical_features = [col for col in numerical_features if col not in columns_to_exclude_from_scaling]\n",
    "\n",
    "    print(f\"\\n스케일링 대상 숫자형 특징: {scaled_numerical_features}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    if scaled_numerical_features: # 스케일링할 특징이 있는 경우에만 적용\n",
    "        # 훈련 데이터의 숫자형 특징에 스케일링을 적용합니다.\n",
    "        # fit_transform을 사용하여 스케일러를 훈련하고 데이터를 변환합니다.\n",
    "        X[scaled_numerical_features] = scaler.fit_transform(X[scaled_numerical_features])\n",
    "\n",
    "        # 테스트 데이터의 숫자형 특징에 훈련된 스케일러를 사용하여 변환합니다.\n",
    "        # 테스트 데이터에는 fit을 다시 호출하지 않고 transform만 사용합니다.\n",
    "        X_test[scaled_numerical_features] = scaler.transform(X_test[scaled_numerical_features])\n",
    "\n",
    "        print(\"숫자형 특징 스케일링 완료.\")\n",
    "        print(\"X (스케일링 후) 샘플:\\n\", X[scaled_numerical_features].head())\n",
    "        print(\"X_test (스케일링 후) 샘플:\\n\", X_test[scaled_numerical_features].head())\n",
    "    else:\n",
    "        print(\"스케일링할 숫자형 특징이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ef23a",
   "metadata": {},
   "source": [
    "\n",
    "    # --- 3. 훈련 데이터셋을 훈련 세트와 검증 세트로 분할 ---\n",
    "    # 모델 학습 및 성능 검증을 위해 훈련 데이터를 훈련 세트와 검증 세트로 나눕니다.\n",
    "    # stratify=y를 사용하여 타겟 변수의 클래스 비율이 훈련 세트와 검증 세트에서 동일하게 유지되도록 합니다.\n",
    "    # random_state를 고정하여 코드 실행 시마다 동일한 분할 결과를 얻도록 합니다.\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\n데이터 분할 완료:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "\n",
    "    # 분할된 데이터의 클래스 비율 확인 (선택 사항)\n",
    "    print(f\"\\ny_train 클래스 비율:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"y_val 클래스 비율:\\n{y_val.value_counts(normalize=True)}\")\n",
    "\n",
    "    # 이 시점에서 X_train, y_train은 모델 학습에 사용될 것이고,\n",
    "    # X_val, y_val은 모델의 성능을 검증하는 데 사용될 것입니다.\n",
    "    # X_test는 최종 예측 파일 생성에 사용될 것입니다.\n",
    "    return X_train, X_val, y_train, y_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48177566",
   "metadata": {},
   "source": [
    "## LightGBM 모델 학습 (LightGBM Model Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28605b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_classification # make_classification 임포트 위치 변경\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 메시지 무시 설정 (LightGBM 등에서 발생하는 경고를 숨기기 위함)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0. 데이터 로드 및 분할 (가상의 데이터, 실제 노트북에서는 이전 섹션에서 로드됨)\n",
    "#    이 섹션의 실행을 위해 임시로 X_train, y_train을 생성합니다.\n",
    "#    실제 환경에서는 이전 단계에서 준비된 'X_train'과 'y_train'을 사용합니다.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fa34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train이 정의되지 않았는지 확인하고 예시 데이터를 생성\n",
    "if 'X_train' not in globals() and 'X_train' not in locals():\n",
    "    print(\"X_train, y_train이 정의되지 않았습니다. 예시 데이터를 생성합니다.\")\n",
    "    # 예시 데이터 생성 (실제 데이터는 이전 섹션에서 로드 및 전처리됨)\n",
    "    X_train_temp, y_train_temp = make_classification(\n",
    "        n_samples=1000, n_features=10, n_informative=5, n_redundant=0,\n",
    "        n_clusters_per_class=1, random_state=42\n",
    "    )\n",
    "    X_train = pd.DataFrame(X_train_temp, columns=[f'feature_{i}' for i in range(X_train_temp.shape[1])])\n",
    "    y_train = pd.Series(y_train_temp)\n",
    "    print(\"예시 데이터 (X_train, y_train) 생성이 완료되었습니다.\")\n",
    "else:\n",
    "    print(\"X_train, y_train이 이미 정의되어 있습니다. 기존 데이터를 사용합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_train class distribution:\\n{y_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. LightGBM 모델 초기화\n",
    "#    `LGBMClassifier`를 사용하여 LightGBM 모델을 초기화합니다.\n",
    "#    `random_state`를 고정하여 결과의 재현성을 확보합니다.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8decfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 1. LightGBM 모델 초기화 ---\")\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "print(\"LightGBM Classifier 모델이 초기화되었습니다.\")\n",
    "print(lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. 하이퍼파라미터 튜닝 (Grid Search with Cross-Validation)\n",
    "#    Grid Search를 사용하여 최적의 하이퍼파라미터를 탐색합니다.\n",
    "#    교차 검증(StratifiedKFold)을 적용하여 모델의 일반화 성능을 평가합니다.\n",
    "#    (주의: 파라미터 그리드를 너무 크게 설정하면 실행 시간이 매우 길어질 수 있습니다.)\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2. 하이퍼파라미터 튜닝 (Grid Search with Cross-Validation) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색할 하이퍼파라미터 그리드 정의\n",
    "# 예시를 위해 간략하게 설정합니다. 실제 환경에서는 더 넓은 범위와 세밀한 값들을 탐색할 수 있습니다.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         # 부스팅 라운드 수\n",
    "    'learning_rate': [0.05, 0.1],       # 학습률\n",
    "    'num_leaves': [20, 31],             # 하나의 트리가 가질 수 있는 최대 잎 노드 수\n",
    "    'max_depth': [5, 7],                # 트리의 최대 깊이\n",
    "    'min_child_samples': [20, 30]       # 리프 노드가 되기 위한 최소한의 데이터 수\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47186e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold 교차 검증 설정 (클래스 불균형 문제를 고려하여 사용)\n",
    "# n_splits: 폴드(fold) 수, shuffle: 데이터 섞기, random_state: 재현성\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV 객체 초기화\n",
    "# estimator: 학습할 모델, param_grid: 탐색할 파라미터 그리드\n",
    "# scoring: 모델 평가 지표 (예: 'accuracy', 'roc_auc', 'f1')\n",
    "# cv: 교차 검증 전략, n_jobs: 병렬 처리할 코어 수 (-1은 모든 코어 사용)\n",
    "# verbose: 진행 상황 출력 레벨 (0: 없음, 1: 요약, 2: 상세)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # 분류 문제이므로 정확도(accuracy)를 사용\n",
    "    cv=skf,\n",
    "    n_jobs=-1,           # 모든 CPU 코어 사용\n",
    "    verbose=2            # 상세한 진행 상황 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GridSearchCV를 시작합니다. (이 과정은 시간이 다소 소요될 수 있습니다.)\")\n",
    "# 훈련 데이터에 GridSearchCV 적용하여 최적의 파라미터 탐색\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- GridSearchCV 결과 ---\")\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(f\"최적의 하이퍼파라미터: {grid_search.best_params_}\")\n",
    "# 최적의 하이퍼파라미터로 얻은 교차 검증 최고 점수 출력\n",
    "print(f\"최고 교차 검증 정확도: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2387c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. 최적의 모델로 최종 학습\n",
    "#    Grid Search를 통해 찾은 최적의 하이퍼파라미터로 LightGBM 모델을 다시 초기화하고,\n",
    "#    전체 훈련 세트(X_train, y_train)를 사용하여 최종 모델을 학습시킵니다.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. 최적의 모델로 최종 학습 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV의 best_estimator_ 속성은 최적의 파라미터로 전체 훈련 세트에 재학습된 모델입니다.\n",
    "# GridSearchCV의 기본 설정 (refit=True)에 따라, 이 모델은 이미 최적의 파라미터로 전체 훈련 데이터에 학습이 완료된 상태입니다.\n",
    "final_lgbm_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6900ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GridSearchCV를 통해 최적화된 LightGBM 모델이 준비되었습니다.\")\n",
    "print(f\"최종 모델 파라미터:\\n{final_lgbm_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. (선택 사항) 학습된 모델의 간단한 성능 확인 (훈련 세트에 대한)\n",
    "#    훈련 세트에 대한 예측 및 분류 리포트를 출력하여 모델의 학습 상태를 확인합니다.\n",
    "#    이는 과적합 여부를 판단하는 데 도움이 될 수 있습니다.\n",
    "#    (주: 실제 모델 평가는 검증 세트나 테스트 세트에서 이루어져야 합니다.)\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4. (선택 사항) 훈련 세트에 대한 모델 성능 확인 ---\")\n",
    "y_train_pred = final_lgbm_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"훈련 세트 정확도: {train_accuracy:.4f}\")\n",
    "print(\"\\n훈련 세트 분류 리포트:\")\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265acb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLightGBM 모델 학습 및 튜닝 섹션이 완료되었습니다.\")\n",
    "# 최종 학습된 모델을 다음 섹션에서 사용할 수 있도록 저장 (예: final_lgbm_model)\n",
    "# globals()['final_lgbm_model'] = final_lgbm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9788036",
   "metadata": {},
   "source": [
    "## 모델 평가 (Model Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from typing import Any, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(\n",
    "    model: Any,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    model_name: str = \"Model\",\n",
    "    target_names: Optional[list] = None,\n",
    "    save_plots: bool = False,\n",
    "    plot_dir: str = \"plots\"\n",
    ") -> Tuple[Dict[str, float], plt.Figure, plt.Figure]:\n",
    "    \"\"\"\n",
    "    Evaluates a binary classification model and visualizes its performance.\n",
    "\n",
    "    Args:\n",
    "        model: Trained scikit-learn compatible classification model (e.g., LightGBM).\n",
    "               Must have `predict` and `predict_proba` methods.\n",
    "        X_val (pd.DataFrame): Validation set features.\n",
    "        y_val (pd.Series): True labels for the validation set.\n",
    "        model_name (str): Name of the model for plot titles and legends.\n",
    "        target_names (Optional[list]): List of target class names for confusion matrix labels.\n",
    "                                       Defaults to ['0', '1'] if None.\n",
    "        save_plots (bool): If True, saves the plots to `plot_dir`.\n",
    "        plot_dir (str): Directory to save plots if `save_plots` is True.\n",
    "                        Defaults to \"plots\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, float], plt.Figure, plt.Figure]:\n",
    "            - A dictionary containing calculated performance metrics.\n",
    "            - Matplotlib Figure object for the Confusion Matrix.\n",
    "            - Matplotlib Figure object for the ROC Curve.\n",
    "    \"\"\"\n",
    "    print(f\"--- {model_name} Model Evaluation ---\")\n",
    "    print(\"1. Performing predictions on the validation set...\")\n",
    "\n",
    "    # Class predictions (0 or 1)\n",
    "    y_pred = model.predict(X_val)\n",
    "    # Predicted probabilities (probability of class 1)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    print(\"Predictions completed.\\n\")\n",
    "\n",
    "    print(\"2. Calculating and printing model performance metrics...\")\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"Precision\": precision_score(y_val, y_pred),\n",
    "        \"Recall\": recall_score(y_val, y_pred),\n",
    "        \"F1-Score\": f1_score(y_val, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_val, y_pred_proba)\n",
    "    }\n",
    "\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"3. Visualizing Confusion Matrix...\")\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    if target_names is None:\n",
    "        target_names = ['0', '1']\n",
    "\n",
    "    cm_fig, ax_cm = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=[f'Predicted {name}' for name in target_names],\n",
    "                yticklabels=[f'Actual {name}' for name in target_names],\n",
    "                ax=ax_cm)\n",
    "    ax_cm.set_xlabel('Predicted Label')\n",
    "    ax_cm.set_ylabel('True Label')\n",
    "    ax_cm.set_title(f'{model_name} Confusion Matrix')\n",
    "\n",
    "    if save_plots:\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        cm_fig.savefig(os.path.join(plot_dir, f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png'))\n",
    "        plt.close(cm_fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Confusion Matrix visualization completed.\\n\")\n",
    "\n",
    "    print(\"4. Visualizing ROC Curve...\")\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "\n",
    "    roc_fig, ax_roc = plt.subplots(figsize=(8, 6))\n",
    "    ax_roc.plot(fpr, tpr, label=f'{model_name} (ROC-AUC = {metrics[\"ROC-AUC\"]:.4f})')\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', label='Random Classifier') # Diagonal line\n",
    "    ax_roc.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax_roc.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax_roc.set_title(f'{model_name} ROC Curve')\n",
    "    ax_roc.legend()\n",
    "    ax_roc.grid(True)\n",
    "\n",
    "    if save_plots:\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        roc_fig.savefig(os.path.join(plot_dir, f'{model_name.lower().replace(\" \", \"_\")}_roc_curve.png'))\n",
    "        plt.close(roc_fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ROC Curve visualization completed.\\n\")\n",
    "    print(f\"{model_name} model evaluation finished.\")\n",
    "\n",
    "    return metrics, cm_fig, roc_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b6a0e",
   "metadata": {},
   "source": [
    "## 예측 및 결과 파일 생성 (Prediction & Submission File Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e47b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assumed Pre-existing Objects ---\n",
    "# The following objects are assumed to be available from previous steps when calling the function:\n",
    "# - lgbm_model: A trained LightGBM model object (e.g., lgb.Booster or lgb.LGBMClassifier).\n",
    "# - X_test_processed: A pandas DataFrame or numpy array containing the preprocessed test features.\n",
    "# - test_df: The original test DataFrame, which must contain the 'PassengerId' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_file(\n",
    "    lgbm_model: lgb.Booster,\n",
    "    X_test_processed: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    prediction_threshold: float = 0.5,\n",
    "    submission_filename: str = 'submission.csv'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a Kaggle-style submission file by performing predictions and formatting the output.\n",
    "\n",
    "    This function takes a trained LightGBM model, preprocessed test data, and the original\n",
    "    test DataFrame (for 'PassengerId') to create a CSV file suitable for submission.\n",
    "\n",
    "    Args:\n",
    "        lgbm_model: The trained LightGBM model object (e.g., lgb.Booster).\n",
    "        X_test_processed: The preprocessed test features (e.g., a pandas DataFrame).\n",
    "        test_df: The original test DataFrame, which must include a 'PassengerId' column.\n",
    "        prediction_threshold: The threshold to convert predicted probabilities into binary\n",
    "                              class labels (0 or 1). Defaults to 0.5.\n",
    "        submission_filename: The name of the CSV file to save the submission to.\n",
    "                             Defaults to 'submission.csv'.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the 'PassengerId' column is missing from 'test_df'.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Prediction and Submission File Generation ---\")\n",
    "\n",
    "    # 1. Prediction\n",
    "    print(\"Predicting survival probabilities on the test dataset...\")\n",
    "    predictions_proba = lgbm_model.predict(X_test_processed)\n",
    "    print(f\"Shape of predicted probabilities array: {predictions_proba.shape}\")\n",
    "    print(f\"First 5 predicted probabilities: {predictions_proba[:5]}\")\n",
    "\n",
    "    print(f\"Converting probabilities to binary predictions using a threshold of {prediction_threshold}...\")\n",
    "    predictions = (predictions_proba > prediction_threshold).astype(int)\n",
    "    print(f\"Shape of binary predictions array: {predictions.shape}\")\n",
    "    print(f\"First 5 binary predictions: {predictions[:5]}\")\n",
    "\n",
    "    # 2. Submission File Generation\n",
    "    print(\"Creating submission DataFrame in Kaggle format...\")\n",
    "\n",
    "    if 'PassengerId' not in test_df.columns:\n",
    "        raise ValueError(\"The 'test_df' DataFrame must contain a 'PassengerId' column.\")\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'],\n",
    "        'Survived': predictions\n",
    "    })\n",
    "\n",
    "    print(f\"Shape of the generated submission DataFrame: {submission_df.shape}\")\n",
    "    print(\"First 5 rows of the submission DataFrame:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "    print(f\"Saving submission DataFrame to '{submission_filename}'...\")\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully created '{submission_filename}'. It is ready for Kaggle submission.\")\n",
    "    print(\"--- Finished Prediction and Submission File Generation ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
